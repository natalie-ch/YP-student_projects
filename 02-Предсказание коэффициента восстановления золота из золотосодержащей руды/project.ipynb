<h1>Содержание<span class="tocSkip"></span></h1>
<div class="toc"><ul class="toc-item"><li><span><a href="#Подготовка-данных" data-toc-modified-id="Подготовка-данных-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Подготовка данных</a></span><ul class="toc-item"><li><span><a href="#Знакомство-с-данными" data-toc-modified-id="Знакомство-с-данными-1.1"><span class="toc-item-num">1.1&nbsp;&nbsp;</span>Знакомство с данными</a></span></li><li><span><a href="#Рассчёт-эффективности-обогащения" data-toc-modified-id="Рассчёт-эффективности-обогащения-1.2"><span class="toc-item-num">1.2&nbsp;&nbsp;</span>Рассчёт эффективности обогащения</a></span></li><li><span><a href="#Анализ-признаков,-недоступных-в-тестовой-выборке" data-toc-modified-id="Анализ-признаков,-недоступных-в-тестовой-выборке-1.3"><span class="toc-item-num">1.3&nbsp;&nbsp;</span>Анализ признаков, недоступных в тестовой выборке</a></span></li><li><span><a href="#Предобработка-данных" data-toc-modified-id="Предобработка-данных-1.4"><span class="toc-item-num">1.4&nbsp;&nbsp;</span>Предобработка данных</a></span><ul class="toc-item"><li><span><a href="#Проверка-на-дубликаты" data-toc-modified-id="Проверка-на-дубликаты-1.4.1"><span class="toc-item-num">1.4.1&nbsp;&nbsp;</span>Проверка на дубликаты</a></span></li><li><span><a href="#Обработка-пропусков" data-toc-modified-id="Обработка-пропусков-1.4.2"><span class="toc-item-num">1.4.2&nbsp;&nbsp;</span>Обработка пропусков</a></span></li></ul></li></ul></li><li><span><a href="#Анализ-данных" data-toc-modified-id="Анализ-данных-2"><span class="toc-item-num">2&nbsp;&nbsp;</span>Анализ данных</a></span><ul class="toc-item"><li><span><a href="#Изменение-концентрации-металлов-(Au,-Ag,-Pb)-на-различных-этапах-очистки" data-toc-modified-id="Изменение-концентрации-металлов-(Au,-Ag,-Pb)-на-различных-этапах-очистки-2.1"><span class="toc-item-num">2.1&nbsp;&nbsp;</span>Изменение концентрации металлов (Au, Ag, Pb) на различных этапах очистки</a></span></li><li><span><a href="#Сравнение-распределения-размеров-гранул-сырья-(feed-size)-на-обучающей-и-тестовой-выборках" data-toc-modified-id="Сравнение-распределения-размеров-гранул-сырья-(feed-size)-на-обучающей-и-тестовой-выборках-2.2"><span class="toc-item-num">2.2&nbsp;&nbsp;</span>Сравнение распределения размеров гранул сырья (feed size) на обучающей и тестовой выборках</a></span></li><li><span><a href="#Исследование-суммарной-концентрации-всех-веществ-на-разных-стадиях" data-toc-modified-id="Исследование-суммарной-концентрации-всех-веществ-на-разных-стадиях-2.3"><span class="toc-item-num">2.3&nbsp;&nbsp;</span>Исследование суммарной концентрации всех веществ на разных стадиях</a></span><ul class="toc-item"><li><span><a href="#До-удаления-выбросов" data-toc-modified-id="До-удаления-выбросов-2.3.1"><span class="toc-item-num">2.3.1&nbsp;&nbsp;</span>До удаления выбросов</a></span></li><li><span><a href="#Удаление-выбросов" data-toc-modified-id="Удаление-выбросов-2.3.2"><span class="toc-item-num">2.3.2&nbsp;&nbsp;</span>Удаление выбросов</a></span></li><li><span><a href="#После-удаления-выбросов" data-toc-modified-id="После-удаления-выбросов-2.3.3"><span class="toc-item-num">2.3.3&nbsp;&nbsp;</span>После удаления выбросов</a></span></li></ul></li></ul></li><li><span><a href="#Поиск-корреляций" data-toc-modified-id="Поиск-корреляций-3"><span class="toc-item-num">3&nbsp;&nbsp;</span>Поиск корреляций</a></span></li><li><span><a href="#Модель" data-toc-modified-id="Модель-4"><span class="toc-item-num">4&nbsp;&nbsp;</span>Модель</a></span><ul class="toc-item"><li><span><a href="#Функция-для-вычисления-итоговой-sMAPE" data-toc-modified-id="Функция-для-вычисления-итоговой-sMAPE-4.1"><span class="toc-item-num">4.1&nbsp;&nbsp;</span>Функция для вычисления итоговой sMAPE</a></span></li><li><span><a href="#Обучение-моделей-и-выбор-лучшей" data-toc-modified-id="Обучение-моделей-и-выбор-лучшей-4.2"><span class="toc-item-num">4.2&nbsp;&nbsp;</span>Обучение моделей и выбор лучшей</a></span></li><li><span><a href="#Тестирование-модели" data-toc-modified-id="Тестирование-модели-4.3"><span class="toc-item-num">4.3&nbsp;&nbsp;</span>Тестирование модели</a></span></li></ul></li><li><span><a href="#Вывод" data-toc-modified-id="Вывод-5"><span class="toc-item-num">5&nbsp;&nbsp;</span>Вывод</a></span></li></ul></div>

## Подготовка данных

import pandas as pd
import numpy as np
from pymystem3 import Mystem
from collections import Counter
from matplotlib import pyplot as plt
from sklearn.metrics import mean_absolute_error
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import RidgeCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Lasso
from scipy import stats as st
from sklearn.dummy import DummyRegressor
import warnings
warnings.filterwarnings("ignore")

### Знакомство с данными

data = pd.read_csv('/datasets/gold_recovery_full.csv')
train = pd.read_csv('/datasets/gold_recovery_train.csv')
test = pd.read_csv('/datasets/gold_recovery_test.csv')

data.head()

data.info()

train.info()

test.info()

print('Тренировочная + тестовая выборки:', len(train) + len(test))
print('Полная выборка:', len(data))
print('Разница между тренировочной и тестовой выборками (количество признаков):', len(train.columns) - len(test.columns))

В данных 87 признаков, все присутствуют в тренировочной выборке, и 34-х нет в тестовой. Разберемся в том, как устроено название признака. 

**Технологический процесс**
*	Rougher feed — исходное сырье
*	Rougher additions (или reagent additions) — флотационные реагенты: Xanthate, Sulphate, Depressant 
	 - Xanthate **— ксантогенат (промотер, или активатор флотации);
	 - Sulphate — сульфат (на данном производстве сульфид натрия);
	 - Depressant — депрессант (силикат натрия).
*	Rougher process (англ. «грубый процесс») — флотация
*	Rougher tails — отвальные хвосты
*	Float banks — флотационная установка
*	Cleaner process — очистка
*	Rougher Au — черновой концентрат золота
*	Final Au — финальный концентрат золота

**Параметры этапов**
*	air amount — объём воздуха
*	fluid levels — уровень жидкости
*	feed size — размер гранул сырья
*	feed rate — скорость подачи


**Наименование признаков** должно быть такое:
**[этап].[тип_параметра].[название_параметра]**

Пример: rougher.input.feed_ag

Возможные значения для блока **[этап]**:
*	rougher — флотация
*	primary_cleaner — первичная очистка
*	secondary_cleaner — вторичная очистка
*	final — финальные характеристики

Возможные значения для блока **[тип_параметра]**:
*	input — параметры сырья
*	output — параметры продукта
*	state — параметры, характеризующие текущее состояние этапа
*	calculation — расчётные характеристики


Теперь **конкретизируем задание**. 

Мы дожны создать модель, которая сможет предсказать коэффициент восстановления золота из золотосодержащей руды - recovery. 

Таких признаков у нас два (на двух этапах - после флотации и итоговый):
* rougher.output.recovery
* final.output.recovery 

Похоже, они и являются **целевыми признаками**.

Эффективность обогащения рассчитывается по формуле
![%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5.png](attachment:%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5.png)
где:
*	C — доля золота в концентрате после флотации/очистки;
*	F — доля золота в сырье/концентрате до флотации/очистки;
*	T — доля золота в отвальных хвостах после флотации/очистки.
Для прогноза коэффициента нужно найти долю золота в концентратах и хвостах. Причём важен не только финальный продукт, но и черновой концентрат.


По формуле мы можем высчитать эти признаки и сравнить с теми, что уже в таблице.

Для решения задачи предлагается использовать метрику **sMAPE** (англ. Symmetric Mean Absolute Percentage Error, «симметричное среднее абсолютное процентное отклонение»), итоговый вариант которой выглядит так 
![%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5.png](attachment:%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5.png)

### Рассчёт эффективности обогащения

Проверьте, что эффективность обогащения рассчитана правильно. Вычислите её на обучающей выборке для признака rougher.output.recovery. Найдите MAE между вашими расчётами и значением признака. Опишите выводы.

**C — доля золота в концентрате после флотации/очистки**
* Доля золота в концентрате после флотации - *rougher.output.concentrate_au*
* Доля золота в концентрате после очистки2 - *final.output.concentrate_au*

**F — доля золота в сырье/концентрате до флотации/очистки**
* доля золота в сырье до флотации - *rougher.input.feed_au*
* доля золота в концентрате до очистки2 - *primary_cleaner.output.concentrate_au*

**T — доля золота в отвальных хвостах после флотации/очистки**
* доля золота в отвальных хвостах после флотации - *rougher.output.tail_au*
* доля золота в отвальных хвостах после очистки2 - *final.output.tail_au*

Формула для рассчета: Recovery = ((C*(F-T))/(F*(C-T)))*100

def Recovery(data, stage):
    try:
        if stage == 'rougher':
            C = data['rougher.output.concentrate_au']
            F = data['rougher.input.feed_au']
            T = data['rougher.output.tail_au']
        elif stage == 'cleaner':
            C = data['final.output.concentrate_au']
            F = data['primary_cleaner.output.concentrate_au']
            T = data['final.output.tail_au']
        recovery = ((C*(F-T))/(F*(C-T)))*100
        return recovery
    except: 
        return f'there is not enough data in the test sample'

Для удобства создадим датасет с удаленными nan в целевых признаках.

recovery_rougher_train = train.dropna().reset_index()
recovery_rougher_train.isna().sum().sum()

recovery_rougher_train['recovery_rougher_formula_calculation'] = Recovery(recovery_rougher_train, 'rougher')

mae = mean_absolute_error(recovery_rougher_train['rougher.output.recovery'], recovery_rougher_train['recovery_rougher_formula_calculation'])
print('mae: {:.20f}'.format(mae))

recovery_rougher_train[['rougher.output.recovery', 'recovery_rougher_formula_calculation']].describe()

recovery_rougher_train['rougher.output.recovery'].hist(alpha=0.5)
recovery_rougher_train['recovery_rougher_formula_calculation'].hist(alpha=0.5)
plt.title('Распределения показателей эффективности обогащения на стадии флотации из предоставленных данных и рассчитанные по формуле')
plt.show()

Средняя абсолютная ошибка несущественна, поэтому можем считать, что данные в rougher.output.recovery верны.

### Анализ признаков, недоступных в тестовой выборке

Проанализируйте признаки, недоступные в тестовой выборке. Что это за параметры? К какому типу относятся?

#функция выделения отличий
def difference(list_1, list_2):
    difference = []
    for i in list_1:
        if i not in list_2:
            difference.append(i)
    return difference

print('Количество недостающих признаков в тестовой выборке:', len(difference(train.columns, test.columns)))

print('А вдруг в тесте есть что-то, чего нет в тренировочной выборке?', len(difference(test.columns, train.columns)))

Разберем на леммы названия этапов и выясним, какие не встречаются в тесте.

def lemm_list(data):
    m = Mystem()
    columns = pd.Series(data.columns)
    lemmas_list = columns.apply(m.lemmatize)
    lemmas_list = sum(lemmas_list, [])
    return lemmas_list

lemmas_train_list = lemm_list(train)
lemmas_test_list = lemm_list(test)

Counter(difference(lemmas_train_list, lemmas_test_list))

Вспомним, как устроено название признака ([этап].[тип_параметра].[название_параметра]). 

В тестовой выборке отсутствуют:
* **final** - полностью отсутствующий этап, состоящий исключительно из **output** (выход вторичной очистки)
* **output** - полностью отсутствующий тип параметра на всех этапах(все концентраты (**concentrate**) и хвосты(**tail**))
* **calculation** - полностью отсутствующий тип параметра на этапе флотации (одним из параметров является au_pb_**ratio**)
* **recovery** (2) - наши целевые признаки

30 output и 4 calculation = 34 отсутствующих признака

Для дальнейшего тестирования модели вернем в тест целевые признаки (позже они уйдут в target)

test = test.merge(data[['date', 'rougher.output.recovery','final.output.recovery']], on=['date'], how='left')
test.info()

### Предобработка данных

#### Проверка на дубликаты

print('Дубликаты в полной выборке:', data.duplicated().sum())
print('Дубликаты в тренировочной выборке:', train.duplicated().sum())
print('Дубликаты в тестовой выборке:', test.duplicated().sum())

Дубликатов нет.

#### Обработка пропусков

Пропусков в данных много. Рассмотрим самые богатые на пропуски признаки.

data.isna().sum().sort_values(ascending=False).head(30).plot(kind='barh', 
                                                             grid=True, 
                                                             figsize=(10,8),  
                                                             label='data')
plt.xlabel('Количество пропущенных значений')
plt.title('Пропуски в data')
plt.show()
train.isna().sum().sort_values(ascending=False).head(30).plot(kind='barh', 
                                                              grid=True, 
                                                              figsize=(10,8), 
                                                              color='green', 
                                                              label='train')
plt.xlabel('Количество пропущенных значений')
plt.title('Пропуски в train')
plt.show()
test.isna().sum().sort_values(ascending=False).head(30).plot(kind='barh', 
                                                             grid=True, 
                                                             figsize=(10,8), 
                                                             color='orange', 
                                                             label='test')
plt.xlabel('Количество пропущенных значений')
plt.legend()
plt.title('Пропуски в test')
plt.show()

print('Процент потерянных данных при удалении всех строк с пропусками по всем признакам:', (
    (len(data) - len(data.dropna())) / len(data))*100, '%')

print('Процент потерянных данны, при удалении всех строк с пропусками по целевым признакам:', (
    (len(data) - len(data.dropna(subset=['rougher.output.recovery', 'final.output.recovery']))) / len(data))*100, '%')

print('Процент потерянных данных при удалении всех строк с пропусками по всем признакам в тренировочной выборке:', (
    (len(train) - len(train.dropna())) / len(train))*100, '%')

print('Процент потерянных данных при удалении всех строк с пропусками по целевым признакам в тренировочной выборке:', (
    (len(train) - len(train.dropna(subset=['rougher.output.recovery', 'final.output.recovery']))) / len(train))*100, '%')

print('Процент потерянных данных при удалении всех строк с пропусками по всем признакам в тестовой выборке:', (
    (len(test) - len(test.dropna())) / len(test))*100, '%')

print('Процент потерянных данных при удалении всех строк с пропусками по целевым признакам в тестовой выборке:', (
    (len(test) - len(test.dropna(subset=['rougher.output.recovery', 'final.output.recovery']))) / len(test))*100, '%')

Изначально удалять 30% данных не хотелось. Была идея удалить все строки с пропусками по целевым признакам (соглашаемся на потерю 16% тренировочной выборки), а остальные заполнить методом fillna, выбрав параметр method='ffill'. Так как все объекты в данных расположены в хронологическом порядке и по условию "соседние по времени параметры часто похожи", попробуем заменить отсутствующие значения на предыдущие присутствующие.
Однако, после обучения моделей было принято решение все пропуски заменить на предыдущие существующие значения, без удаления части.

Проверим только сначала отдельно на пропуски столбец с временем.

print('Пропуски в столбце datetime data:', data['date'].isna().sum())
print('Пропуски в столбце datetime train:', train['date'].isna().sum())
print('Пропуски в столбце datetime test:', test['date'].isna().sum())

print('Контрольное суммарное количество пропусков в тренировочном датасете:', train.isna().sum().sum())

data = data.fillna(method='ffill')
train = train.fillna(method='ffill')
test = test.fillna(method='ffill')

print('Контрольное суммарное количество пропусков в тренировочном датасете:', train.isna().sum().sum())

## Анализ данных

### Изменение концентрации металлов (Au, Ag, Pb) на различных этапах очистки

Посмотрите, как меняется концентрация металлов (Au, Ag, Pb) на различных этапах очистки. Опишите выводы.

**На входе:**
* Сырьё:
     - rougher.input.feed_ag
     - rougher.input.feed_pb
     - rougher.input.feed_au

**Выход после флотации:**
* Концентраты:
     - rougher.output.concentrate_ag
     - rougher.output.concentrate_pb
     - rougher.output.concentrate_au
* Хвосты:
     - rougher.output.tail_ag
     - rougher.output.tail_pb
     - rougher.output.tail_au

**Выход после первичной очистки:**
* Концентраты:
     - primary_cleaner.output.concentrate_ag
     - primary_cleaner.output.concentrate_pb
     - primary_cleaner.output.concentrate_au
* Хвосты:
     - primary_cleaner.output.tail_ag
     - primary_cleaner.output.tail_pb
     - primary_cleaner.output.tail_au 

**Выход после вторичной очистки (финальный):**
* Концентраты:
     - final.output.concentrate_ag
     - final.output.concentrate_pb
     - final.output.concentrate_au 
* Хвосты:
     - final.output.tail_ag
     - final.output.tail_pb
     - final.output.tail_au 

Для удобства создадим отдельные датасеты со сравниваемыми признаками.

data_au_concentrate = data[['rougher.input.feed_au', 
                            'rougher.output.concentrate_au', 
                            'primary_cleaner.output.concentrate_au', 
                            'final.output.concentrate_au']]
data_ag_concentrate = data[['rougher.input.feed_ag', 
                            'rougher.output.concentrate_ag', 
                            'primary_cleaner.output.concentrate_ag', 
                            'final.output.concentrate_ag']]
data_pb_concentrate = data[['rougher.input.feed_pb', 
                            'rougher.output.concentrate_pb', 
                            'primary_cleaner.output.concentrate_pb', 
                            'final.output.concentrate_pb']]

data_au_tail = data[['rougher.input.feed_au', 
                     'rougher.output.tail_au', 
                     'primary_cleaner.output.tail_au', 
                     'final.output.tail_au']]
data_ag_tail = data[['rougher.input.feed_ag', 
                     'rougher.output.tail_ag', 
                     'primary_cleaner.output.tail_ag', 
                     'final.output.tail_ag']]
data_pb_tail = data[['rougher.input.feed_pb', 
                     'rougher.output.tail_pb', 
                     'primary_cleaner.output.tail_pb', 
                     'final.output.tail_pb']]

Построим гистограммы и боксплоты, чтобы сравнить распределения.

def histogram(data, name, feature):
    data.plot(kind='hist', 
                         bins=99, 
                         figsize=(15,6), 
                         grid=True, 
                         alpha=0.5)
    plt.title(name)
    plt.xlabel(feature)
    plt.show()
    
    plt.figure(figsize=(15,6))
    sns.boxplot(data=data)
    plt.xlabel('Этапы')
    plt.ylabel(feature)
    plt.title(name)
    plt.show()

histogram(data_au_concentrate, 'Концентрация золота на входе и после каждого из этапов', 'Концентрация')
histogram(data_ag_concentrate, 'Концентрация серебра на входе и после каждого из этапов', 'Концентрация')
histogram(data_pb_concentrate, 'Концентрация свинца на входе и после каждого из этапов', 'Концентрация')

Концентрация золота в продукте на каждом из последующих этапов закономерно возрастает, вместе с тем растёт и дисперсия. Нулевые концентрации есть и на финальном выходе.
Концентрация серебра высока после флотации, затем поэтапно уменьшается. 
Концентрация свинца увеличивается до этапа очистки. После первичной и вторичной очистки медианно она одинаковая, однако после второй очистки становится меньше дисперсия.

histogram(data_au_tail, 'Концентрация золота на входе и в хвостах после каждого из этапов', 'Концентрация')
histogram(data_ag_tail, 'Концентрация серебра на входе и в хвостах после каждого из этапов', 'Концентрация')
histogram(data_pb_tail, 'Концентрация свинца на входе и в хвостах после каждого из этапов', 'Концентрация')

Ситуация в отвальных хвостах тоже выглядит логичной. Концентрация всех веществ максимально низкой становится в хвостах после флотации, возрастает после первичной очистки и уменьшается (вместе с дисперсией) в финале. Заметное отличие у золота, концентрация которого после очисток в хвостах изменяется не так заметно.

### Сравнение распределения размеров гранул сырья (feed size) на обучающей и тестовой выборках

*	Сравните распределения размеров гранул сырья на обучающей и тестовой выборках.
*	Если распределения сильно отличаются друг от друга, оценка модели будет неправильной.


* train['rougher.input.feed_size'] - размер гранул перед флотацией
* train['primary_cleaner.input.feed_size'] - размер гранул перед очисткой


* test['rougher.input.feed_size'] - размер гранул перед флотацией
* test['primary_cleaner.input.feed_size'] - размер гранул перед очисткой

rougher_feed_size = train[['rougher.input.feed_size']].reset_index(drop=True) 
rougher_feed_size['rougher.input.feed_size_test'] = test['rougher.input.feed_size'].reset_index(drop=True)
rougher_feed_size.rename(columns={'rougher.input.feed_size': 'rougher.input.feed_size_train'}, inplace=True)

cleaner_feed_size = train[['primary_cleaner.input.feed_size']].reset_index(drop=True) 
cleaner_feed_size['primary_cleaner.input.feed_size_test'] = test['primary_cleaner.input.feed_size'].reset_index(drop=True)
cleaner_feed_size.rename(columns={'primary_cleaner.input.feed_size': 'primary_cleaner.input.feed_size_train'}, inplace=True)

histogram(rougher_feed_size, 'Размеры гранул до флотации в тренировочной и тестовой выбрках', 'Размер гранул')
histogram(cleaner_feed_size, 'Размеры гранул до первичной очистки в тренировочной и тестовой выбрках', 'Размер гранул')

rougher_feed_size.describe()

cleaner_feed_size.describe()

Распределения размеров гранул в тренировочной и тестовых выборках на обоих этапах очень похожи. На этапе очистки гранулы уже существенно мельче, и разброс их размеров не такой большой, как до флотации. 

### Исследование суммарной концентрации всех веществ на разных стадиях

*	Исследуйте суммарную концентрацию всех веществ на разных стадиях: в сырье, в черновом и финальном концентратах.
*	Заметили аномальные значения в суммарном распределении или нет?
*	Если они есть, стоит ли их удалять из обеих выборок?
*	Опишите выводы и удалите аномалии.


**На входе:**
* Сырьё:
     - rougher.input.feed_ag
     - rougher.input.feed_pb
     - rougher.input.feed_au
     - rougher.input.feed_sol

**Выход после флотации:**
* Концентраты:
     - rougher.output.concentrate_ag
     - rougher.output.concentrate_pb
     - rougher.output.concentrate_au
     - rougher.output.concentrate_sol
* Хвосты:
     - rougher.output.tail_ag
     - rougher.output.tail_pb
     - rougher.output.tail_au
     - rougher.output.tail_sol

**Выход после первичной очистки:**
* Концентраты:
     - primary_cleaner.output.concentrate_ag
     - primary_cleaner.output.concentrate_pb
     - primary_cleaner.output.concentrate_au
     - primary_cleaner.output.concentrate_sol
* Хвосты:
     - primary_cleaner.output.tail_ag
     - primary_cleaner.output.tail_pb
     - primary_cleaner.output.tail_au
     - primary_cleaner.output.tail_sol

**Выход после вторичной очистки (финальный):**
* Концентраты:
     - final.output.concentrate_ag
     - final.output.concentrate_pb
     - final.output.concentrate_au 
     - final.output.concentrate_sol
* Хвосты:
     - final.output.tail_ag
     - final.output.tail_pb
     - final.output.tail_au 
     - final.output.tail_sol

Суммируем столбцы с показателями концентрации на каждом из этапов.

def sum_concentrate(data):
    data['before_rougher'] = data[['rougher.input.feed_ag', 
                                   'rougher.input.feed_pb', 
                                   'rougher.input.feed_au', 
                                   'rougher.input.feed_sol']].sum(axis='columns') 

    data['after_rougher'] = data[['rougher.output.concentrate_ag', 
                                  'rougher.output.concentrate_pb', 
                                  'rougher.output.concentrate_au', 
                                  'rougher.output.concentrate_sol', 
                                  'rougher.output.tail_ag', 
                                  'rougher.output.tail_pb', 
                                  'rougher.output.tail_au', 
                                  'rougher.output.tail_sol']].sum(axis='columns')

    data['after_primary_cleaner'] = data[['primary_cleaner.output.concentrate_ag', 
                                          'primary_cleaner.output.concentrate_pb', 
                                          'primary_cleaner.output.concentrate_au', 
                                          'primary_cleaner.output.concentrate_sol', 
                                          'primary_cleaner.output.tail_ag', 
                                          'primary_cleaner.output.tail_pb', 
                                          'primary_cleaner.output.tail_au', 
                                          'primary_cleaner.output.tail_sol']].sum(axis='columns')
    
    data['after_secondary_cleaner'] = data[['final.output.concentrate_ag', 
                                            'final.output.concentrate_pb', 
                                            'final.output.concentrate_au', 
                                            'final.output.concentrate_sol', 
                                            'final.output.tail_ag', 
                                            'final.output.tail_pb', 
                                            'final.output.tail_au', 
                                            'final.output.tail_sol']].sum(axis='columns')
    return data

data = sum_concentrate(data)
train = sum_concentrate(train)

test_full = test.merge(data[['date', 'before_rougher', 'after_rougher', 'after_primary_cleaner', 'after_secondary_cleaner']], on='date', how='left')

#### До удаления выбросов

histogram(data[['before_rougher', 'after_rougher', 'after_primary_cleaner', 'after_secondary_cleaner']], 'Cуммарная концентрация всех веществ на разных этапах', 'Суммарная концентрация')

histogram(train[['before_rougher', 'after_rougher', 'after_primary_cleaner', 'after_secondary_cleaner']], 'Cуммарная концентрация всех веществ на разных этапах', 'Суммарная концентрация')

histogram(test_full[['before_rougher', 'after_rougher', 'after_primary_cleaner', 'after_secondary_cleaner']], 'Cуммарная концентрация всех веществ на разных этапах', 'Суммарная концентрация')

Медианно суммарные концентрации веществ на трех этапах схожи, однако, дисперсии сильно отличаются, видим много выбросов.

#### Удаление выбросов

data_before = len(data)
train_before = len(train)
test_full_before = len(test_full)

def outlier_remove(data, features):
    for feature in features:
        data_r_threshold = data[feature].quantile(0.75) + (data[feature].quantile(0.75) - data[feature].quantile(0.25))*1.5
        data_l_threshold = data[feature].quantile(0.25) - (data[feature].quantile(0.75) - data[feature].quantile(0.25))*1.5
        data[(data[feature] > data_l_threshold) & (data[feature] < data_r_threshold)]
        data = data[(data[feature] > data_l_threshold) & (data[feature] < data_r_threshold)]
    return data

Дата нам больше не пригодится и будет лишней в обучении.

data = data.drop(['date'],axis=1)
train = train.drop(['date'],axis=1)
test = test.drop(['date'],axis=1)

data = outlier_remove(data, ['before_rougher','after_rougher', 'after_primary_cleaner', 'after_secondary_cleaner'])
train = outlier_remove(train, ['before_rougher', 'after_rougher', 'after_primary_cleaner', 'after_secondary_cleaner'])
test_full = outlier_remove(test_full, ['before_rougher', 'after_rougher', 'after_primary_cleaner', 'after_secondary_cleaner'])

print('Доля оставшейся полной выборки', len(data)/data_before)
print('Доля оставшейся тренировочной выборки', len(train)/train_before)
print('Доля оставшейся тестовой выборки', len(test_full)/test_full_before)

Удалим выбросы из test

test = test.query('index in @test_full.index')

#### После удаления выбросов

histogram(data[['before_rougher', 'after_rougher', 'after_primary_cleaner', 'after_secondary_cleaner']], 'Cуммарная концентрация всех веществ на разных этапах', 'Суммарная концентрация')

histogram(train[['before_rougher', 'after_rougher', 'after_primary_cleaner', 'after_secondary_cleaner']], 'Cуммарная концентрация всех веществ на разных этапах', 'Суммарная концентрация')

histogram(test_full[['before_rougher', 'after_rougher', 'after_primary_cleaner', 'after_secondary_cleaner']], 'Cуммарная концентрация всех веществ на разных этапах', 'Суммарная концентрация')

После удаления выбросов дисперсии стали не такими разнообразными. Распределения стали более схожими.

## Поиск корреляций

Так как признаков много, отберём из них те, коэффициент корреляции между которыми выше 0.5.

def correlations(data):
    correlations = []
    for feature in list(data.columns):
        for name in list(data.columns):
            correlation = data[feature].corr(data[name])
            if 0.5 <= abs(correlation) < 0.9999999:
                correlations.append([correlation, feature, name])
    return correlations

print('Коррелирующих признаков в тренировочном датасете:', len(correlations(train))/2)

train_corr = pd.DataFrame(correlations(train), columns=['coefficient', 'columns_1', 'columns_2']).sort_values(by='coefficient', ascending=False)
train_corr.style.background_gradient(cmap='coolwarm')

correlation_threshold = 0.98

train_corr[abs(train_corr['coefficient'])>correlation_threshold]

potentially_remove = list(train_corr[abs(train_corr['coefficient'])>correlation_threshold]['columns_2'])
potentially_remove

Возможно, при обучении модели будет осмысленным убрать часть features, список которых будет зависеть от correlation_threshold, чтобы избежать снижения производительности обобщения данных из-за высокой дисперсии и меньшей интерпретируемости модели.

def correlations_target(data):
    correlations = []
    for feature in list(data.columns):
        for name in list(data[['rougher.output.recovery', 'final.output.recovery']].columns):
            correlation = data[feature].corr(data[name])
            if 0.5 <= abs(correlation) < 0.9999999:
                correlations.append([correlation, feature, name])
    return correlations

pd.DataFrame(correlations_target(train), columns=['coefficient', 'columns', 'target']).sort_values(by='coefficient', ascending=False).style.background_gradient(cmap='coolwarm')

Целевые признаки коррелируют друг с другом и обратно с двумя хвостами после флотации, а так же с суммарной концентрацией веществ до флотации.

## Модель

### Функция для вычисления итоговой sMAPE

![%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5.png](attachment:%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5.png)

![%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5.png](attachment:%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5.png)

def sMAPE_score(target, predictions):
    target_array = np.array(target)
    num = abs(predictions - target_array)
    den = (abs(predictions) + abs(target_array))/2
    sMAPE = (num/den).sum()*100*(1/len(target))
    return sMAPE

def final_sMAPE_score(sMAPE_scores_rougher,sMAPE_scores_final):
    final_sMAPE = (pd.Series(sMAPE_scores_rougher)*0.25 + pd.Series(sMAPE_scores_final)*0.75)
    return final_sMAPE

### Обучение моделей и выбор лучшей

Так как в тренировочной выборке присутствуют признаки, являющиеся ответами (выходами) в процессе, мы не станем пускать их в обучение модели. Помним, что всех output нет в тестовой выборке. Воспользуемся этим - создадим одинаковый набор признаков для тестовой и тренировочной выборок. 

test_columns = test.columns
train = train[test_columns]

А теперь поэкспериментируем с удалением сильно коррелирующих признаков из выборок

len(train.columns)

len(test.columns)

#функция, отвечающая за удаление одного из высококоррелирующих признаков
def del_column(data):
    for column in data.columns:
        if column in potentially_remove:
            data = data.drop([column],  axis=1)
    return data

train = del_column(train)
test = del_column(test)

len(train.columns)

len(test.columns)

train = train.reset_index(drop=True)
test = test.reset_index(drop=True)

Выделим целевые признаки.

features = train.drop(['rougher.output.recovery', 'final.output.recovery'],  axis=1)
target_rougher = train['rougher.output.recovery']
target_final = train['final.output.recovery']

#функция, отвечающая за стандартизацию, обучение и оценку
def fit_scores(model, features_train, target_train, features_valid, target_valid):
    
    scaler = StandardScaler()
    scaler.fit(features_train) 
    features_train = scaler.transform(features_train)
    features_valid = scaler.transform(features_valid)
    
    model = model.fit(features_train, target_train)    
    predictions = model.predict(features_valid)
    smape = sMAPE_score(target_valid, predictions)
    mae=mean_absolute_error(target_valid,predictions)
    
    return model, scaler, smape, mae

#функция, отвечающая за кросс-валидацию и финальную оценку
def model_cross_val(model_type):
    
    sMAPE_scores_rougher = []
    mae_scores_rougher = []

    sMAPE_scores_final = []
    mae_scores_final = []

    sample_size = int(len(train)/6)

    for i in range(0, len(train), sample_size):
    
        valid_indexes=features.iloc[i:i+sample_size].index
        train_indexes=features.query('index not in @valid_indexes').index
        
        features_train = features.iloc[train_indexes]
        features_valid = features.iloc[valid_indexes]
    
        target_rougher_train = target_rougher.iloc[train_indexes]
        target_rougher_valid = target_rougher.iloc[valid_indexes]
        
        target_final_train = target_final.iloc[train_indexes]
        target_final_valid = target_final.iloc[valid_indexes]
    
        #model = Pipeline([('Scaler', StandardScaler()), ('model', model_type)])
        model = model_type
        
        model_rougher, scaler_rougher, smape_rougher, mae_rougher = fit_scores(model, 
                                                               features_train, target_rougher_train, 
                                                               features_valid, target_rougher_valid)
        sMAPE_scores_rougher.append(smape_rougher)
        mae_scores_rougher.append(mae_rougher)
        
        
        model_final, scaler_final, smape_final, mae_final = fit_scores(model, 
                                                         features_train, target_final_train, 
                                                         features_valid, target_final_valid)
        sMAPE_scores_final.append(smape_final)
        mae_scores_final.append(mae_final)
        
        final_sMAPE = final_sMAPE_score(sMAPE_scores_rougher, sMAPE_scores_final)
        final_sMAPE_mean = final_sMAPE.mean()
      
    
    print('sMAPE_scores_rougher:', np.mean(sMAPE_scores_rougher))
    print('mae_scores_rougher:', np.mean(mae_scores_rougher))
    print()
    print('sMAPE_scores_final:', np.mean(sMAPE_scores_final))
    print('mae_scores_final:', np.mean(mae_scores_final))
    print()
    print('final_sMAPE:')
    print(final_sMAPE)
    print()
    print('final_sMAPE_std:', final_sMAPE.std())
    print()
    print('final_sMAPE_mean:', final_sMAPE_mean)
    
    return model_rougher, model_final, scaler_rougher, scaler_final

Обучим модели

model_rougher_linear, model_final_linear, scaler_rougher_linear, scaler_final_linear = model_cross_val(LinearRegression())

model_rougher_forest, model_final_forest, scaler_rougher_forest, scaler_final_forest = model_cross_val(RandomForestRegressor())

model_rougher_lasso, model_final_lasso, scaler_rougher_lasso, scaler_final_lasso = model_cross_val(Lasso())

model_rougher_ridgecv, model_final_ridgecv, scaler_rougher_ridgecv, scaler_final_ridgecv = model_cross_val(RidgeCV())

Для сравнения обучим dummy модель.

model_rougher_dummy, model_final_dummy, scaler_rougher_dummy, scaler_final_dummy = model_cross_val(DummyRegressor())

По сочетанию средней final sMAPE и разброса final_sMAPE про кросс-валидации лучшей оказалась модель lasso. Её и пустим на тест. 

### Тестирование модели

Выделим целевые признаки из тестовой выборки.

test_features = test.drop(['rougher.output.recovery', 'final.output.recovery'],  axis=1)
test_target_rougher = test['rougher.output.recovery']
test_target_final = test['final.output.recovery']

test_features_rougher = scaler_rougher_lasso.transform(test_features)

test_predictions_rougher = model_rougher_lasso.predict(test_features_rougher)
mae_score_rougher = mean_absolute_error(test_target_rougher,test_predictions_rougher)
sMAPE_score_rougher = sMAPE_score(test_target_rougher,test_predictions_rougher)
print('mae_score_rougher:', mae_score_rougher)
print('sMAPE_score_rougher:', sMAPE_score_rougher)

test_features_final = scaler_final_lasso.transform(test_features)

test_predictions_final = model_final_lasso.predict(test_features_final)
mae_score_final = mean_absolute_error(test_target_final,test_predictions_final)
sMAPE_score_final = sMAPE_score(test_target_final,test_predictions_final)
print('mae_score_final:', mae_score_final)
print('sMAPE_score_final:', sMAPE_score_final)

final_sMAPE_test = final_sMAPE_score(sMAPE_score_rougher,sMAPE_score_final)
print('final_sMAPE_test:')
print(final_sMAPE_test.mean())

И сравним с dummy моделью.

test_features_rougher = scaler_rougher_dummy.transform(test_features)

test_predictions_rougher = model_rougher_dummy.predict(test_features_rougher)
mae_score_rougher = mean_absolute_error(test_target_rougher,test_predictions_rougher)
sMAPE_score_rougher = sMAPE_score(test_target_rougher,test_predictions_rougher)
print('mae_score_rougher:', mae_score_rougher)
print('sMAPE_score_rougher:', sMAPE_score_rougher)

test_features_final = scaler_final_dummy.transform(test_features)

test_predictions_final = model_final_dummy.predict(test_features_final)
mae_score_final = mean_absolute_error(test_target_final,test_predictions_final)
sMAPE_score_final = sMAPE_score(test_target_final,test_predictions_final)
print('mae_score_final:', mae_score_final)
print('sMAPE_score_final:', sMAPE_score_final)

final_sMAPE_test = final_sMAPE_score(sMAPE_score_rougher,sMAPE_score_final)
print('final_sMAPE_test:')
print(final_sMAPE_test.mean())

При тестировании final sMAPE выбранной модели оказался лучше (меньше) результата dummy модели.

## Вывод

При исследовании данных технологического процесса очистки руды мы проверили точность вычисления коэффициента восстановления золота из руды, исследовали признаки на разных этапах очистки, обучали модели и вычисляли параметр sMAPE для целевых признаков. В итоге была выбрана модель с наименьшим sMAPE при кросс-валидации - линейная модель Lasso.
